<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Search Engine</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<h1>Search Engine</h1>
						<p>Results and Approach</p>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Content -->
							<section id="content" class="main">
								<span class="image main"><img src="images/searchengine.jpeg" alt="" width="100" height="500" /></span>
								<h2>Approach</h2>
                                <p>
                                    To build the search engine, we performed the data preprocessing steps same as what were done in the plagiarism checker. The search engine is build on 7395 news articles from the same dataset.
                                    We removed the unwanted data from text content and the subject columns in the dataset. </p>
                                <img src="images/data_prep.png" alt=""  width="1000" height="250" />
                                <br>
                                    <p>In order to build the Document search engine we used the Google Universal sentence encoder model.
                                        The Universal Sentence Encoder encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.
                                    The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. 
                                    It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector.
                                    This encoder differs from word level embedding models in that it's trained on a number of natural language prediction tasks that require modeling the meaning of word sequences rather than just individual words
                                    </p>

                                    <h3>Semantic Similarity</h3>
                                    <p>Semantic similarity is a measure of the degree to which two pieces of text carry the same meaning. This is broadly useful in obtaining good coverage over the numerous ways that a thought can be expressed using language without needing to manually enumerate them.
                                    </p>
                                    <img src="images/example-similarity.png" alt=""  width="1000" height="250" />

                                    <h3>Classification</h3>
                                    <p>The Universal Sentence Encoder was partially trained with custom text classification tasks in mind. These kinds of classifiers can be trained to perform a wide variety of classification tasks often with a very small amount of labeled examples.</p>
                                    <img src="images/example-classification.png" alt=""  width="1000" height="200" />
                                    <p>We have trained the dataset at batch-wise because it takes a long time to execution to generate the graph of the dataset. so better to train batch-wise data.</p>
                                    <h3>Function for document search:</h3>
                                    <img src="images/search.png" alt=""  width="900" height="350" />
                                    <br><br>
                                    <h2>Results</h2>
                                    <p>Searching "climate change"....</p>
                                    <img src="images/climate_change.png" alt=""  width="600" height="400" />
                                    <br><br>

                                    <p>Searching "immigrants"....</p>
                                    <img src="images/immigrants.png" alt=""  width="600" height="400" />
                                    <br><br>

                                    <p>Searching "President Trump Campaign"....</p>
                                    <img src="images/trumppy.png" alt=""  width="600" height="400" />
									<br><br>
									<p>Searching "economic crisis in america"....</p>
                                    <img src="images/africa.png" alt=""  width="600" height="400" />

									<br><br>
				     <img src="images/table.png" alt=""  width="1000" height="170" />
				<p>We defined three levels for the search engine with score: 2 – Relevant, 1- Somewhat Relevant, 0 – Not Relevant. For each above mentioned query, we checked where in each category the document falls in. We found out the best possible case based on the query and calculated NDCG score values at position 3,6 and 10 </p>
                            
                            </section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h2>References</h2>
							<ul>
								<li><a target="_blank" rel="noopener noreferrer" href="https://tfhub.dev/google/universal-sentence-encoder/4">Universal Sentence Encoder</a></li>
								
							 
							</ul>
						</section>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
